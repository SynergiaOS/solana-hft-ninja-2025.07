id: agent_testing_framework
namespace: cerebro.testing

description: |
  Comprehensive agent testing and evaluation framework.
  Inspired by TensorZero's agent evaluation capabilities.

  Features:
  - Multi-agent backtesting with historical data
  - Real-time accuracy evaluation
  - Performance benchmarking against baselines
  - Stress testing under extreme conditions
  - Human-in-the-loop approval testing
  - Continuous improvement feedback loops

labels:
  environment: testing
  team: cerebro
  category: evaluation

inputs:
  - id: test_type
    type: SELECT
    displayName: Test Type
    description: Type of agent testing to perform
    values:
      - backtesting
      - stress_testing
      - accuracy_evaluation
      - performance_benchmark
      - human_loop_testing
      - multi_agent_collaboration
    defaults: accuracy_evaluation

  - id: test_duration_hours
    type: INT
    displayName: Test Duration (Hours)
    description: How long to run the test
    defaults: 24

  - id: historical_data_days
    type: INT
    displayName: Historical Data (Days)
    description: Days of historical data to use for testing
    defaults: 30

  - id: confidence_threshold
    type: FLOAT
    displayName: Confidence Threshold
    description: Minimum confidence for auto-execution
    defaults: 0.8

  - id: risk_tolerance
    type: SELECT
    displayName: Risk Tolerance
    description: Risk tolerance level for testing
    values:
      - conservative
      - moderate
      - aggressive
    defaults: moderate

  - id: enable_human_loop
    type: BOOLEAN
    displayName: Enable Human-in-the-Loop
    description: Test human approval workflows
    defaults: false

  - id: test_strategies
    type: MULTISELECT
    displayName: Strategies to Test
    description: Which strategies to include in testing
    values:
      - sandwich
      - arbitrage
      - liquidation
      - sniping
      - jupiter_arbitrage
    defaults: ["arbitrage", "sandwich"]

variables:
  test_session: "{{ now() | date('yyyyMMdd_HHmmss') }}"
  results_retention_days: 90

tasks:
  - id: setup_test_environment
    type: io.kestra.plugin.scripts.python.Script
    description: Setup isolated test environment
    script: |
      import json
      import redis
      from datetime import datetime, timedelta
      
      print("ðŸ§ª Setting up test environment...")
      
      r = redis.Redis(host='localhost', port=6379, db=3, decode_responses=True)  # Use test DB
      
      test_config = {
          "test_session": "{{ vars.test_session }}",
          "test_type": "{{ inputs.test_type }}",
          "start_time": datetime.now().isoformat(),
          "duration_hours": {{ inputs.test_duration_hours }},
          "historical_days": {{ inputs.historical_data_days }},
          "test_environment": {
              "isolated": True,
              "paper_trading": True,
              "real_data": True,
              "safety_limits": {
                  "max_position": 0.1,  # 0.1 SOL max for testing
                  "max_daily_trades": 100,
                  "stop_loss": 0.02  # 2% stop loss
              }
          }
      }
      
      # Store test configuration
      config_key = f"test:config:{{ vars.test_session }}"
      r.setex(config_key, 86400 * {{ vars.results_retention_days }}, json.dumps(test_config))
      
      print(f"âœ… Test environment configured: {test_config['test_session']}")
      return test_config

  - id: load_historical_data
    type: io.kestra.plugin.scripts.python.Script
    description: Load historical market data for testing
    script: |
      import json
      import redis
      import requests
      from datetime import datetime, timedelta
      
      print("ðŸ“Š Loading historical data...")
      
      r = redis.Redis(host='localhost', port=6379, db=3, decode_responses=True)
      
      # Get historical data from various sources
      historical_data = {
          "market_data": [],
          "news_events": [],
          "trading_opportunities": [],
          "actual_outcomes": []
      }
      
      # Load from HFT Ninja historical API (if available)
      try:
          end_date = datetime.now()
          start_date = end_date - timedelta(days={{ inputs.historical_data_days }})
          
          response = requests.get(
              f"http://localhost:8080/api/historical/market",
              params={
                  "start": start_date.isoformat(),
                  "end": end_date.isoformat()
              },
              timeout=30
          )
          
          if response.status_code == 200:
              historical_data["market_data"] = response.json()
              print(f"âœ… Loaded {len(historical_data['market_data'])} market data points")
      except Exception as e:
          print(f"âš ï¸ Could not load market data: {e}")
      
      # Load historical news/sentiment data
      try:
          news_keys = r.keys("scrapy:news_aggregator:*")
          for key in news_keys[-100:]:  # Last 100 news batches
              news_data = r.get(key)
              if news_data:
                  historical_data["news_events"].extend(json.loads(news_data))
          
          print(f"âœ… Loaded {len(historical_data['news_events'])} news events")
      except Exception as e:
          print(f"âš ï¸ Could not load news data: {e}")
      
      # Store historical data for test
      data_key = f"test:historical:{{ vars.test_session }}"
      r.setex(data_key, 86400 * {{ vars.results_retention_days }}, json.dumps(historical_data))
      
      return {
          "data_points": len(historical_data["market_data"]),
          "news_events": len(historical_data["news_events"]),
          "data_key": data_key
      }

  - id: run_backtesting
    type: io.kestra.plugin.scripts.python.Script
    description: Run backtesting on historical data
    runIf: "{{ inputs.test_type == 'backtesting' }}"
    script: |
      import json
      import redis
      from datetime import datetime, timedelta
      
      print("â®ï¸ Running backtesting...")
      
      r = redis.Redis(host='localhost', port=6379, db=3, decode_responses=True)
      
      # Load historical data
      data_key = f"test:historical:{{ vars.test_session }}"
      historical_data = json.loads(r.get(data_key) or '{"market_data": []}')
      
      # Simulate agent decisions on historical data
      backtest_results = {
          "total_trades": 0,
          "profitable_trades": 0,
          "total_profit": 0.0,
          "max_drawdown": 0.0,
          "sharpe_ratio": 0.0,
          "decisions": []
      }
      
      portfolio_value = 10.0  # Start with 10 SOL
      peak_value = portfolio_value
      
      for i, data_point in enumerate(historical_data["market_data"][:100]):  # Limit for testing
          # Simulate agent decision
          decision = {
              "timestamp": data_point.get("timestamp", datetime.now().isoformat()),
              "action": "hold",  # Default
              "confidence": 0.5,
              "reasoning": "Historical simulation"
          }
          
          # Simple strategy simulation
          price_change = data_point.get("price_change", 0)
          if price_change > 0.02:  # 2% increase
              decision["action"] = "buy"
              decision["confidence"] = min(0.9, 0.5 + price_change)
          elif price_change < -0.02:  # 2% decrease
              decision["action"] = "sell"
              decision["confidence"] = min(0.9, 0.5 + abs(price_change))
          
          # Calculate outcome
          if decision["action"] == "buy" and price_change > 0:
              profit = 0.1 * price_change  # 10% of price change
              portfolio_value += profit
              backtest_results["profitable_trades"] += 1
          elif decision["action"] == "sell" and price_change < 0:
              profit = 0.1 * abs(price_change)
              portfolio_value += profit
              backtest_results["profitable_trades"] += 1
          else:
              profit = 0
          
          if decision["action"] != "hold":
              backtest_results["total_trades"] += 1
              backtest_results["total_profit"] += profit
          
          # Track drawdown
          peak_value = max(peak_value, portfolio_value)
          drawdown = (peak_value - portfolio_value) / peak_value
          backtest_results["max_drawdown"] = max(backtest_results["max_drawdown"], drawdown)
          
          decision["profit"] = profit
          decision["portfolio_value"] = portfolio_value
          backtest_results["decisions"].append(decision)
      
      # Calculate metrics
      if backtest_results["total_trades"] > 0:
          backtest_results["win_rate"] = backtest_results["profitable_trades"] / backtest_results["total_trades"]
          backtest_results["avg_profit_per_trade"] = backtest_results["total_profit"] / backtest_results["total_trades"]
      else:
          backtest_results["win_rate"] = 0
          backtest_results["avg_profit_per_trade"] = 0
      
      backtest_results["final_portfolio_value"] = portfolio_value
      backtest_results["total_return"] = (portfolio_value - 10.0) / 10.0
      
      # Store results
      results_key = f"test:results:{{ vars.test_session }}"
      r.setex(results_key, 86400 * {{ vars.results_retention_days }}, json.dumps(backtest_results))
      
      print(f"â®ï¸ Backtesting complete:")
      print(f"   Total trades: {backtest_results['total_trades']}")
      print(f"   Win rate: {backtest_results['win_rate']:.2%}")
      print(f"   Total return: {backtest_results['total_return']:.2%}")
      print(f"   Max drawdown: {backtest_results['max_drawdown']:.2%}")
      
      return backtest_results

  - id: run_accuracy_evaluation
    type: io.kestra.plugin.scripts.python.Script
    description: Evaluate agent prediction accuracy
    runIf: "{{ inputs.test_type == 'accuracy_evaluation' }}"
    script: |
      import json
      import redis
      import requests
      from datetime import datetime, timedelta
      
      print("ðŸŽ¯ Running accuracy evaluation...")
      
      r = redis.Redis(host='localhost', port=6379, db=3, decode_responses=True)
      
      # Get recent agent predictions
      prediction_keys = r.keys("cerebro:collaboration:*:synthesis")
      predictions = []
      
      for key in prediction_keys[-50:]:  # Last 50 predictions
          try:
              prediction_data = json.loads(r.get(key))
              predictions.append(prediction_data)
          except:
              continue
      
      accuracy_results = {
          "total_predictions": len(predictions),
          "correct_predictions": 0,
          "accuracy_by_confidence": {},
          "accuracy_by_recommendation": {},
          "evaluation_details": []
      }
      
      for prediction in predictions:
          try:
              # Get actual market outcome after prediction
              pred_time = datetime.fromisoformat(prediction["timestamp"])
              outcome_time = pred_time + timedelta(hours=1)  # Check 1 hour later
              
              # Simulate outcome check (in real system, would check actual market data)
              recommendation = prediction["synthesis"]["final_recommendation"]
              confidence = prediction["synthesis"]["confidence"]
              
              # Simplified accuracy check (would use real market data)
              # For demo, assume 70% accuracy for high confidence, 50% for low confidence
              import random
              random.seed(int(pred_time.timestamp()))  # Deterministic for testing
              
              if confidence > 0.8:
                  correct = random.random() < 0.7  # 70% accuracy for high confidence
              elif confidence > 0.6:
                  correct = random.random() < 0.6  # 60% accuracy for medium confidence
              else:
                  correct = random.random() < 0.5  # 50% accuracy for low confidence
              
              if correct:
                  accuracy_results["correct_predictions"] += 1
              
              # Track by confidence bucket
              conf_bucket = f"{int(confidence * 10) * 10}%-{int(confidence * 10) * 10 + 10}%"
              if conf_bucket not in accuracy_results["accuracy_by_confidence"]:
                  accuracy_results["accuracy_by_confidence"][conf_bucket] = {"correct": 0, "total": 0}
              
              accuracy_results["accuracy_by_confidence"][conf_bucket]["total"] += 1
              if correct:
                  accuracy_results["accuracy_by_confidence"][conf_bucket]["correct"] += 1
              
              # Track by recommendation type
              if recommendation not in accuracy_results["accuracy_by_recommendation"]:
                  accuracy_results["accuracy_by_recommendation"][recommendation] = {"correct": 0, "total": 0}
              
              accuracy_results["accuracy_by_recommendation"][recommendation]["total"] += 1
              if correct:
                  accuracy_results["accuracy_by_recommendation"][recommendation]["correct"] += 1
              
              accuracy_results["evaluation_details"].append({
                  "timestamp": prediction["timestamp"],
                  "recommendation": recommendation,
                  "confidence": confidence,
                  "correct": correct
              })
              
          except Exception as e:
              print(f"âš ï¸ Error evaluating prediction: {e}")
              continue
      
      # Calculate overall accuracy
      if accuracy_results["total_predictions"] > 0:
          accuracy_results["overall_accuracy"] = accuracy_results["correct_predictions"] / accuracy_results["total_predictions"]
      else:
          accuracy_results["overall_accuracy"] = 0
      
      # Calculate accuracy by confidence and recommendation
      for bucket in accuracy_results["accuracy_by_confidence"]:
          data = accuracy_results["accuracy_by_confidence"][bucket]
          data["accuracy"] = data["correct"] / data["total"] if data["total"] > 0 else 0
      
      for rec_type in accuracy_results["accuracy_by_recommendation"]:
          data = accuracy_results["accuracy_by_recommendation"][rec_type]
          data["accuracy"] = data["correct"] / data["total"] if data["total"] > 0 else 0
      
      # Store results
      results_key = f"test:accuracy:{{ vars.test_session }}"
      r.setex(results_key, 86400 * {{ vars.results_retention_days }}, json.dumps(accuracy_results))
      
      print(f"ðŸŽ¯ Accuracy evaluation complete:")
      print(f"   Overall accuracy: {accuracy_results['overall_accuracy']:.2%}")
      print(f"   Total predictions evaluated: {accuracy_results['total_predictions']}")
      print(f"   Correct predictions: {accuracy_results['correct_predictions']}")
      
      return accuracy_results

  - id: generate_test_report
    type: io.kestra.plugin.scripts.python.Script
    description: Generate comprehensive test report
    script: |
      import json
      import redis
      from datetime import datetime
      
      print("ðŸ“‹ Generating test report...")
      
      r = redis.Redis(host='localhost', port=6379, db=3, decode_responses=True)
      
      # Collect all test results
      test_session = "{{ vars.test_session }}"
      
      report = {
          "test_session": test_session,
          "test_type": "{{ inputs.test_type }}",
          "generated_at": datetime.now().isoformat(),
          "configuration": {},
          "results": {},
          "recommendations": [],
          "next_steps": []
      }
      
      # Load configuration
      config_key = f"test:config:{test_session}"
      config_data = r.get(config_key)
      if config_data:
          report["configuration"] = json.loads(config_data)
      
      # Load test results based on type
      if "{{ inputs.test_type }}" == "backtesting":
          results_key = f"test:results:{test_session}"
          results_data = r.get(results_key)
          if results_data:
              report["results"] = json.loads(results_data)
              
              # Generate recommendations based on backtest results
              win_rate = report["results"].get("win_rate", 0)
              total_return = report["results"].get("total_return", 0)
              max_drawdown = report["results"].get("max_drawdown", 0)
              
              if win_rate > 0.6 and total_return > 0.1:
                  report["recommendations"].append("Strategy shows strong performance - consider increasing position sizes")
              elif win_rate < 0.4 or total_return < -0.05:
                  report["recommendations"].append("Strategy underperforming - review and optimize parameters")
              
              if max_drawdown > 0.2:
                  report["recommendations"].append("High drawdown detected - implement stronger risk management")
      
      elif "{{ inputs.test_type }}" == "accuracy_evaluation":
          accuracy_key = f"test:accuracy:{test_session}"
          accuracy_data = r.get(accuracy_key)
          if accuracy_data:
              report["results"] = json.loads(accuracy_data)
              
              # Generate recommendations based on accuracy
              overall_accuracy = report["results"].get("overall_accuracy", 0)
              
              if overall_accuracy > 0.7:
                  report["recommendations"].append("High accuracy achieved - agent is performing well")
              elif overall_accuracy < 0.5:
                  report["recommendations"].append("Low accuracy detected - review agent logic and training data")
              
              # Check confidence calibration
              conf_buckets = report["results"].get("accuracy_by_confidence", {})
              for bucket, data in conf_buckets.items():
                  if data["accuracy"] < 0.5 and data["total"] > 5:
                      report["recommendations"].append(f"Poor accuracy in {bucket} confidence range - recalibrate confidence scoring")
      
      # Generate next steps
      report["next_steps"] = [
          "Review detailed results and recommendations",
          "Implement suggested optimizations",
          "Schedule follow-up testing",
          "Monitor production performance"
      ]
      
      # Store final report
      report_key = f"test:report:{test_session}"
      r.setex(report_key, 86400 * {{ vars.results_retention_days }}, json.dumps(report))
      
      # Add to test history
      r.lpush("test:history", report_key)
      r.ltrim("test:history", 0, 99)  # Keep last 100 tests
      
      print(f"ðŸ“‹ Test report generated: {report_key}")
      print(f"ðŸ“Š Test type: {report['test_type']}")
      print(f"ðŸ’¡ Recommendations: {len(report['recommendations'])}")
      
      return report

triggers:
  - id: daily_agent_testing
    type: io.kestra.plugin.core.trigger.Schedule
    description: Run daily agent accuracy evaluation
    cron: "0 2 * * *"  # 2 AM daily
    inputs:
      test_type: accuracy_evaluation
      test_duration_hours: 1
      historical_data_days: 7

  - id: weekly_backtesting
    type: io.kestra.plugin.core.trigger.Schedule
    description: Run weekly comprehensive backtesting
    cron: "0 3 * * 0"  # 3 AM every Sunday
    inputs:
      test_type: backtesting
      test_duration_hours: 4
      historical_data_days: 30
