id: scrapy_intelligence_flow
namespace: cerebro.intelligence

description: |
  Comprehensive Solana intelligence gathering using Scrapy spiders.
  Monitors Discord, news sources, DEX activity, and project health.

labels:
  environment: development
  team: cerebro
  category: intelligence

inputs:
  - id: spider_name
    type: SELECT
    displayName: Spider to Run
    description: Select which spider to execute
    values:
      - discord_monitor
      - news_aggregator
      - project_auditor
      - dex_monitor
      - all
    defaults: all

  - id: max_items
    type: INT
    displayName: Max Items
    description: Maximum number of items to scrape
    defaults: 100

  - id: output_format
    type: SELECT
    displayName: Output Format
    description: Format for scraped data output
    values:
      - json
      - jsonl
      - csv
    defaults: json

variables:
  scrapy_project_path: "{{ workingDir }}/cerebro/scrapy_spiders"
  output_dir: "{{ workingDir }}/scraped_data"
  timestamp: "{{ now() | date('yyyy-MM-dd_HH-mm-ss') }}"

tasks:
  - id: setup_environment
    type: io.kestra.plugin.scripts.shell.Commands
    description: Setup Scrapy environment and dependencies
    commands:
      - echo "Setting up Scrapy environment..."
      - cd {{ vars.scrapy_project_path }}
      - source ../venv/bin/activate
      - pip install --quiet scrapy redis
      - mkdir -p {{ vars.output_dir }}
      - echo "Environment ready"

  - id: run_discord_monitor
    type: io.kestra.plugin.scripts.shell.Commands
    description: Monitor Discord servers for Solana ecosystem activity
    runIf: "{{ inputs.spider_name == 'discord_monitor' or inputs.spider_name == 'all' }}"
    commands:
      - cd {{ vars.scrapy_project_path }}
      - source ../venv/bin/activate
      - echo "Starting Discord monitoring..."
      - scrapy crawl discord_monitor -s CLOSESPIDER_ITEMCOUNT={{ inputs.max_items }} -o {{ vars.output_dir }}/discord_{{ vars.timestamp }}.{{ inputs.output_format }}
      - echo "Discord monitoring completed"

  - id: run_news_aggregator
    type: io.kestra.plugin.scripts.shell.Commands
    description: Aggregate news from crypto media sources
    runIf: "{{ inputs.spider_name == 'news_aggregator' or inputs.spider_name == 'all' }}"
    commands:
      - cd {{ vars.scrapy_project_path }}
      - source ../venv/bin/activate
      - echo "Starting news aggregation..."
      - scrapy crawl news_aggregator -s CLOSESPIDER_ITEMCOUNT={{ inputs.max_items }} -o {{ vars.output_dir }}/news_{{ vars.timestamp }}.{{ inputs.output_format }}
      - echo "News aggregation completed"

  - id: run_project_auditor
    type: io.kestra.plugin.scripts.shell.Commands
    description: Audit Solana projects for risk indicators
    runIf: "{{ inputs.spider_name == 'project_auditor' or inputs.spider_name == 'all' }}"
    commands:
      - cd {{ vars.scrapy_project_path }}
      - source ../venv/bin/activate
      - echo "Starting project auditing..."
      - scrapy crawl project_auditor -s CLOSESPIDER_ITEMCOUNT={{ inputs.max_items }} -o {{ vars.output_dir }}/projects_{{ vars.timestamp }}.{{ inputs.output_format }}
      - echo "Project auditing completed"

  - id: run_dex_monitor
    type: io.kestra.plugin.scripts.shell.Commands
    description: Monitor DEX activity and new token listings
    runIf: "{{ inputs.spider_name == 'dex_monitor' or inputs.spider_name == 'all' }}"
    commands:
      - cd {{ vars.scrapy_project_path }}
      - source ../venv/bin/activate
      - echo "Starting DEX monitoring..."
      - scrapy crawl dex_monitor -s CLOSESPIDER_ITEMCOUNT={{ inputs.max_items }} -o {{ vars.output_dir }}/dex_{{ vars.timestamp }}.{{ inputs.output_format }}
      - echo "DEX monitoring completed"

  - id: process_scraped_data
    type: io.kestra.plugin.scripts.python.Script
    description: Process and analyze scraped data
    script: |
      import json
      import os
      import glob
      from datetime import datetime
      
      output_dir = "{{ vars.output_dir }}"
      timestamp = "{{ vars.timestamp }}"
      
      print(f"Processing scraped data from {output_dir}")
      
      # Find all output files from this run
      pattern = f"{output_dir}/*_{timestamp}.json"
      files = glob.glob(pattern)
      
      total_items = 0
      summary = {
          'timestamp': timestamp,
          'files_processed': len(files),
          'total_items': 0,
          'by_spider': {},
          'alerts': []
      }
      
      for file_path in files:
          spider_name = os.path.basename(file_path).split('_')[0]
          
          try:
              with open(file_path, 'r') as f:
                  data = json.load(f)
                  
              if isinstance(data, list):
                  item_count = len(data)
              else:
                  item_count = 1
                  
              summary['by_spider'][spider_name] = item_count
              total_items += item_count
              
              # Check for alerts in the data
              if isinstance(data, list):
                  for item in data:
                      if isinstance(item, dict) and 'alerts' in item:
                          summary['alerts'].extend(item['alerts'])
              elif isinstance(data, dict) and 'alerts' in data:
                  summary['alerts'].extend(data['alerts'])
                  
          except Exception as e:
              print(f"Error processing {file_path}: {e}")
      
      summary['total_items'] = total_items
      
      # Save summary
      summary_file = f"{output_dir}/summary_{timestamp}.json"
      with open(summary_file, 'w') as f:
          json.dump(summary, f, indent=2)
      
      print(f"Processing complete:")
      print(f"- Total items: {total_items}")
      print(f"- Files processed: {len(files)}")
      print(f"- Alerts generated: {len(summary['alerts'])}")
      print(f"- Summary saved to: {summary_file}")
      
      # Output for Kestra
      return {
          'total_items': total_items,
          'files_processed': len(files),
          'alerts_count': len(summary['alerts']),
          'summary_file': summary_file
      }

  - id: send_to_dragonfly
    type: io.kestra.plugin.scripts.python.Script
    description: Send high-priority data to DragonflyDB
    script: |
      import json
      import redis
      import glob
      from datetime import datetime
      
      # Connect to DragonflyDB
      try:
          r = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
          r.ping()
          print("Connected to DragonflyDB")
      except Exception as e:
          print(f"Failed to connect to DragonflyDB: {e}")
          return {'status': 'error', 'message': str(e)}
      
      output_dir = "{{ vars.output_dir }}"
      timestamp = "{{ vars.timestamp }}"
      
      # Process files and send to DragonflyDB
      pattern = f"{output_dir}/*_{timestamp}.json"
      files = glob.glob(pattern)
      
      items_sent = 0
      
      for file_path in files:
          spider_name = os.path.basename(file_path).split('_')[0]
          
          try:
              with open(file_path, 'r') as f:
                  data = json.load(f)
              
              # Send to DragonflyDB with appropriate keys
              if isinstance(data, list):
                  for i, item in enumerate(data):
                      key = f"scrapy:{spider_name}:{timestamp}:{i}"
                      r.setex(key, 3600, json.dumps(item))  # 1 hour TTL
                      items_sent += 1
              else:
                  key = f"scrapy:{spider_name}:{timestamp}"
                  r.setex(key, 3600, json.dumps(data))
                  items_sent += 1
              
              # Add to spider-specific list
              list_key = f"scrapy:list:{spider_name}"
              r.lpush(list_key, f"{timestamp}")
              r.ltrim(list_key, 0, 99)  # Keep last 100 runs
              
          except Exception as e:
              print(f"Error processing {file_path}: {e}")
      
      print(f"Sent {items_sent} items to DragonflyDB")
      return {'items_sent': items_sent, 'status': 'success'}

triggers:
  - id: daily_intelligence_gathering
    type: io.kestra.plugin.core.trigger.Schedule
    description: Run intelligence gathering daily at 6 AM
    cron: "0 6 * * *"
    inputs:
      spider_name: all
      max_items: 200
      output_format: json

  - id: hourly_dex_monitoring
    type: io.kestra.plugin.core.trigger.Schedule
    description: Monitor DEX activity every hour during trading hours
    cron: "0 8-22 * * *"
    inputs:
      spider_name: dex_monitor
      max_items: 50
      output_format: json

  - id: news_monitoring
    type: io.kestra.plugin.core.trigger.Schedule
    description: Monitor news every 4 hours
    cron: "0 */4 * * *"
    inputs:
      spider_name: news_aggregator
      max_items: 30
      output_format: json
